# LLM Embeddings Test

A Python tool for comparing embeddings generated by different language models. This project demonstrates how various models represent text at the token level, showing both contextual embeddings (from transformer models) and static word embeddings.

## Features

- **Interactive sentence input** - Enter any sentence to see how different models represent it
- **Token-level analysis** - View embeddings for individual tokens/words
- **Multiple model types**:
  - **Transformer models**: BERT, Sentence-Transformers, RoBERTa (contextual embeddings)
  - **Word embeddings**: GloVe (static embeddings)
- **Formatted output** - Clean table display showing tokens and their embedding values
- **Error handling** - Graceful fallback when models are unavailable

## Installation

1. Clone the repository:
```bash
git clone https://github.com/lukerf89/LLM-Embeddings-Test.git
cd LLM-Embeddings-Test
```

2. Install required dependencies:
```bash
pip install transformers torch gensim numpy
```

Note: You may need to downgrade NumPy for compatibility:
```bash
pip install "numpy<2"
```

## Usage

Run the script and enter a sentence when prompted:

```bash
python3 embeddings.py
```

Example output:
```
Enter a sentence to generate embeddings: Hello world!

=================================================================
TRANSFORMER MODELS (Contextual Embeddings)
=================================================================

Model: bert-base-uncased
Token           Embedding (first 5 values)
-----------------------------------------------------------------
[CLS]           [-0.142, 0.133, -0.129, -0.171, -0.483...]
hello           [-0.350, 0.104, 0.624, -0.799, -0.226...]
world           [-0.245, -0.157, 0.694, -0.375, 0.011...]
...
```

## Models Included

### Transformer Models (Contextual Embeddings)
- **bert-base-uncased**: 768-dimensional embeddings from Google's BERT
- **sentence-transformers/all-MiniLM-L6-v2**: 384-dimensional sentence embeddings
- **roberta-base**: 768-dimensional embeddings from Facebook's RoBERTa

### Word Embedding Models (Static Embeddings)
- **glove-wiki-gigaword-100**: 100-dimensional GloVe embeddings (downloads on first use)

## Key Differences

**Contextual vs Static Embeddings:**
- **Contextual** (Transformer models): Token representations change based on surrounding context
- **Static** (Word embeddings): Each word has a fixed representation regardless of context

**Token Handling:**
- **Transformers**: Use subword tokenization with special tokens (`[CLS]`, `[SEP]`, `<s>`, `</s>`)
- **Word embeddings**: Simple word-level tokenization

## Requirements

- Python 3.7+
- Internet connection (for downloading GloVe model on first use)
- ~2GB free space (for model downloads)

## Architecture

The codebase consists of:
- `get_token_embeddings()`: Handles transformer model inference
- `get_word_embeddings()`: Handles word embedding lookup
- `display_token_table()`: Formats output tables
- Error handling for network connectivity issues

## Notes

- GloVe model (~350MB) downloads automatically on first use
- Transformer models are included with the `transformers` package
- Out-of-vocabulary words in GloVe are handled with zero vectors
- All models run locally after initial setup